{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk6Nl7aM9mLX6UCTmvg5q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Slayerma/mid-term/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTpkXiR9KOFg",
        "outputId": "aaef2b3c-3777-4ada-b6b8-725fe3fbe3ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n",
            "Collecting syllables\n",
            "  Downloading syllables-1.0.9-py3-none-any.whl (15 kB)\n",
            "Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n",
            "  Downloading cmudict-1.0.16-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<7.0,>=5.1 (from syllables)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict<2.0.0,>=1.0.11->syllables) (6.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=5.1->syllables) (3.17.0)\n",
            "Installing collected packages: importlib-metadata, cmudict, syllables\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "Successfully installed cmudict-1.0.16 importlib-metadata-6.11.0 syllables-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install openpyxl\n",
        "!pip install textblob\n",
        "!python -m textblob.download_corpora\n",
        "!pip install syllables\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import syllables  # Install using: pip install syllables\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract article text from a given URL\n",
        "def extract_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract only article title and text\n",
        "        article_title = soup.title.text if soup.title else ''\n",
        "        article_text = ' '.join([p.text for p in soup.find_all('p')])\n",
        "\n",
        "        return article_title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {url}: {str(e)}\")\n",
        "        return '', ''\n",
        "\n",
        "# Function to perform textual analysis and compute variables\n",
        "def analyze_text(text):\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    positive_score = blob.sentiment.polarity\n",
        "    negative_score = -positive_score\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n",
        "\n",
        "    sentences = blob.sentences\n",
        "    avg_sentence_length = sum(len(sentence.words) for sentence in sentences) / len(sentences)\n",
        "    complex_words = [word for word in blob.words if syllables.estimate(word) > 2]\n",
        "    percentage_complex_words = (len(complex_words) / len(blob.words)) * 100\n",
        "\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    avg_words_per_sentence = len(blob.words) / len(sentences)\n",
        "    complex_word_count = len(complex_words)\n",
        "    word_count = len(blob.words)\n",
        "    syllables_per_word = sum(syllables.estimate(word) for word in blob.words) / len(blob.words)\n",
        "\n",
        "    personal_pronouns = sum(1 for word in blob.words if word.lower() in ['i', 'me', 'my', 'mine', 'myself'])\n",
        "\n",
        "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
        "\n",
        "    return [positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "            avg_sentence_length, percentage_complex_words, fog_index,\n",
        "            avg_words_per_sentence, complex_word_count, word_count,\n",
        "            syllables_per_word, personal_pronouns, avg_word_length]\n",
        "\n",
        "# Load input URLs from Excel file\n",
        "input_df = pd.read_excel('/content/Input.xlsx')\n",
        "\n",
        "# Iterate through each row and perform data extraction and analysis\n",
        "output_data = []\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    # Extract text from the URL\n",
        "    article_title, article_text = extract_text(url)\n",
        "\n",
        "    # Perform textual analysis\n",
        "    variables = analyze_text(article_text)\n",
        "\n",
        "    # Append the results to the output_data list\n",
        "    output_data.append([url_id, *variables])\n",
        "\n",
        "# Create output DataFrame\n",
        "output_columns = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
        "                  'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
        "                  'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
        "                  'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "\n",
        "output_df = pd.DataFrame(output_data, columns=output_columns)\n",
        "\n",
        "# Save the output DataFrame to Excel file\n",
        "output_df.to_excel('Output_Data.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import syllables\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the NLTK stopwords dataset\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to extract article text from a given URL\n",
        "def extract_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract only article title and text\n",
        "        article_title = soup.title.text if soup.title else ''\n",
        "        article_text = ' '.join([p.text for p in soup.find_all('p')])\n",
        "\n",
        "        return article_title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {url}: {str(e)}\")\n",
        "        return '', ''\n",
        "\n",
        "# Function to clean text using stop words lists\n",
        "def clean_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "# Function to perform textual analysis and compute variables\n",
        "def analyze_text(text):\n",
        "    cleaned_text = clean_text(text)\n",
        "    blob = TextBlob(cleaned_text)\n",
        "\n",
        "    # Sentimental Analysis\n",
        "    positive_score = len([word for word in blob.words if word in positive_words])\n",
        "    negative_score = len([word for word in blob.words if word in negative_words])\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(blob.words) + 0.000001)\n",
        "\n",
        "    # Analysis of Readability\n",
        "    sentences = blob.sentences\n",
        "    avg_sentence_length = sum(len(sentence.words) for sentence in sentences) / len(sentences)\n",
        "    complex_words = [word for word in blob.words if syllables.estimate(word) > 2]\n",
        "    percentage_complex_words = (len(complex_words) / len(blob.words)) * 100\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Additional Analysis\n",
        "    avg_words_per_sentence = len(blob.words) / len(sentences)\n",
        "    complex_word_count = len(complex_words)\n",
        "    word_count = len(blob.words)\n",
        "    syllables_per_word = sum(syllables.estimate(word) for word in blob.words) / len(blob.words)\n",
        "\n",
        "    personal_pronouns = len(re.findall(r'\\b(?:I|we|my|ours|us)\\b', cleaned_text, flags=re.IGNORECASE))\n",
        "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
        "\n",
        "    return [positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "            avg_sentence_length, percentage_complex_words, fog_index,\n",
        "            avg_words_per_sentence, complex_word_count, word_count,\n",
        "            syllables_per_word, personal_pronouns, avg_word_length]\n",
        "\n",
        "# Load positive and negative words from master dictionary\n",
        "positive_words = set(pd.read_csv('/content/positive-words.txt', header=None, encoding='latin1')[0])\n",
        "negative_words = set(pd.read_csv('/content/negative-words.txt', header=None, encoding='latin1')[0])\n",
        "\n",
        "# Load input URLs from Excel file\n",
        "input_df = pd.read_excel('/content/Input.xlsx')\n",
        "\n",
        "# Iterate through each row and perform data extraction and analysis\n",
        "output_data = []\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    # Extract text from the URL\n",
        "    article_title, article_text = extract_text(url)\n",
        "\n",
        "    # Perform textual analysis\n",
        "    variables = analyze_text(article_text)\n",
        "\n",
        "    # Append the results to the output_data list\n",
        "    output_data.append([url_id, *variables])\n",
        "\n",
        "# Create output DataFrame\n",
        "output_columns = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
        "                  'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
        "                  'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
        "                  'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "\n",
        "output_df = pd.DataFrame(output_data, columns=output_columns)\n",
        "\n",
        "# Save the output DataFrame to Excel file\n",
        "output_df.to_excel('/content/Output_Data.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CkoI8yhy7If",
        "outputId": "d07d9091-fe08-489c-e82e-b4e2d1932156"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}